# ID2223-lab2
Model we fine-tuned for lab2 of course ID2223  
To check out this file on hugging face: [click here](https://huggingface.co/rkwsuper/lora_model/tree/main)  
To see the user interface: [click here](https://huggingface.co/spaces/someday12/lab)  

Task2: To improve model performance, we can  
**a) Model-Centric Approach:**  
- Adjust the hyperparameters, for example, train for **more epochs** or change the **batch size** to balance updates and efficiency.  
- Choose a **different base model** for fine-tuning, such as using a larger or newer model.  
- Introduce **dropout layers** to prevent overfitting by randomly deactivating some neurons during training.  

**b) Data-Centric Approach:**  
- Perform **data augmentation** by making small changes to sentences, such as rephrasing, word order changes to add variety.  
- **Standardize the data**: Prepare all data in a consistent format and clean irrelevant or duplicate data.  
- Use **updated and fresh data** to ensure the model learns from the latest information.

A work done by groupC


